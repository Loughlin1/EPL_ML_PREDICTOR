{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014-2015', '2015-2016', '2016-2017', '2017-2018', '2018-2019', '2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024']\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a list of seasons you want to scrape\n",
    "def generate_seasons(start_year, end_year):\n",
    "    seasons = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        season_start = str(year)\n",
    "        season_end = str(year + 1)\n",
    "        season = f\"{season_start}-{season_end}\"\n",
    "        seasons.append(season)\n",
    "    return seasons\n",
    "\n",
    "start_year = 2014\n",
    "end_year = 2023\n",
    "\n",
    "seasons = generate_seasons(start_year, end_year)\n",
    "print(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_seasons(seasons):\n",
    "    for season in seasons:\n",
    "        url = 'https://fbref.com/en/comps/9/' + season + '/schedule/' + season + '-Premier-League-Scores-and-Fixtures'\n",
    "        df = pd.read_html(url, attrs={\"id\": str(\"sched_\" + season + \"_9_1\")})[0]\n",
    "        df.to_csv( './data/' + season[:4] + '-' + season[7:9] + '.csv')\n",
    "        time.sleep(30)\n",
    "\n",
    "# scrape_seasons(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_season_stats(url):\n",
    "    try:\n",
    "        df = pd.read_html(url, attrs={\"id\": \"matchlogs_for\"})[0]\n",
    "        return df if not df.empty else None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_teams_stats(seasons, squad_id, team_name):\n",
    "    urls = ['https://fbref.com/en/squads/' + squad_id + '/' + season + '/matchlogs/c9/shooting/' + team_name + '-Match-Logs-Premier-League' for season in seasons]\n",
    "    \n",
    "    dfs = []\n",
    "    for url in urls:\n",
    "        df = scrape_season_stats(url)\n",
    "        time.sleep(30)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "    if dfs:\n",
    "        df = pd.concat(dfs, ignore_index=False)\n",
    "        df = df.droplevel(level=0, axis=1)\n",
    "        df.to_csv('../data/shooting_data/' + team_name + '.csv')\n",
    "    else:\n",
    "        print(f\"No valid data for team {team_name} in seasons {seasons}\")\n",
    "\n",
    "def scrape_all_teams_stats(seasons, team_ids):\n",
    "    counter = 0\n",
    "    for team, id in team_ids.items():\n",
    "        scrape_teams_stats(seasons, id, team)\n",
    "        time.sleep(30)\n",
    "        if counter == 3:\n",
    "            time.sleep(120)\n",
    "        counter +=1\n",
    "\n",
    "team_ids = json.load(open('../encoders/team_ids.json'))\n",
    "# team_ids.items()\n",
    "# scrape_all_teams_stats(seasons, team_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = json.load(open('../encoders/training_teams.json'))\n",
    "\n",
    "def rolling_stats(df, team_name):\n",
    "    df.dropna(subset=['Date'], inplace=True) \n",
    "\n",
    "    # Getting rolling averages\n",
    "    cols = [\"GF\", \"GA\", \"Sh\", \"SoT\", \"PK\",\"PKatt\"]\n",
    "    new_cols = [f\"{c}_rolling\" for c in cols]\n",
    "\n",
    "    rolling_stats = df[cols].rolling(3, closed='left').mean()\n",
    "    df[new_cols] = rolling_stats\n",
    "    # df = df.dropna(subset=new_cols)\n",
    "\n",
    "    df.loc[df['Venue'] == 'Home', 'HomeTeam'] = team_name\n",
    "    df.loc[df['Venue'] == 'Home', 'AwayTeam'] = df['Opponent']\n",
    "    df.loc[df['Venue'] == 'Away', 'HomeTeam'] = df['Opponent']\n",
    "    df.loc[df['Venue'] == 'Away', 'AwayTeam'] = team_name\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format=\"%Y-%m-%d\")\n",
    "    \n",
    "    # Check if 'Venue' column has correct entries\n",
    "    if 'Home' not in df['Venue'].unique() or 'Away' not in df['Venue'].unique():\n",
    "        print(\"Error: 'Venue' column does not contain expected values.\")\n",
    "        return df\n",
    "    \n",
    "    rolling_home_cols = [f\"{c}_rolling_h\" for c in cols]\n",
    "    rolling_away_cols = [f\"{c}_rolling_a\" for c in cols]\n",
    "\n",
    "    df.loc[df['Venue'] == 'Home', rolling_home_cols] = df.loc[df['Venue'] == 'Home', new_cols].values\n",
    "    df.loc[df['Venue'] == 'Away', rolling_away_cols] = df.loc[df['Venue'] == 'Away', new_cols].values\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_rolling_stats(teams):\n",
    "    rolling_dfs = []\n",
    "\n",
    "    for team in teams:\n",
    "        df = pd.read_csv('shooting_data/'+ team + '.csv')\n",
    "        rolling_df = rolling_stats(df, teams[team])\n",
    "        rolling_dfs.append(rolling_df)\n",
    "    \n",
    "    combined_df = pd.concat(rolling_dfs, ignore_index=False)\n",
    "    merged_df = combined_df.groupby(['Date', 'HomeTeam', 'AwayTeam'], as_index=False).first()\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# merge_rolling_stats(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_season_stats(url):\n",
    "    try:\n",
    "        df = pd.read_html(url, attrs={\"id\": \"matchlogs_for\"})[0]\n",
    "        return df if not df.empty else None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_teams_stats(seasons, squad_id, team_name):\n",
    "    urls = ['https://fbref.com/en/squads/' + squad_id + '/' + season + '/matchlogs/c9/shooting/' + team_name + '-Match-Logs-Premier-League' for season in seasons]\n",
    "    \n",
    "    dfs = []\n",
    "    for url in urls:\n",
    "        df = scrape_season_stats(url)\n",
    "        time.sleep(10)\n",
    "        if df is not None:\n",
    "            dfs.append(df)\n",
    "    if dfs:\n",
    "        df = pd.concat(dfs, ignore_index=False)\n",
    "        df = df.droplevel(level=0, axis=1)\n",
    "        df.to_csv('../data/shooting_data_2024/' + team_name + '.csv')\n",
    "        print(f\"Exported {team_name} to /data/shooting_data_2024/{team_name}.csv\")\n",
    "    else:\n",
    "        print(f\"No valid data for team {team_name} in seasons {seasons}\")\n",
    "\n",
    "def scrape_all_teams_stats(seasons, team_ids):\n",
    "    counter = 0\n",
    "    for team, id in team_ids.items():\n",
    "        scrape_teams_stats(seasons, id, team)\n",
    "        if counter == 3:\n",
    "            time.sleep(10)\n",
    "        counter +=1\n",
    "\n",
    "team_ids_2024_25 = {\n",
    "    'Arsenal': '18bb7c10',\n",
    "    'Aston-Villa': '8602292d',\n",
    "    'Bournemouth': '4ba7cbea',\n",
    "    'Brentford': 'cd051869',\n",
    "    'Brighton-and-Hove-Albion': 'd07537b9',\n",
    "    'Chelsea': 'cff3d9bb',\n",
    "    'Crystal-Palace': '47c64c55',\n",
    "    'Everton': 'd3fd31cc',\n",
    "    'Fulham': 'fd962109',\n",
    "    'Ipswich-Town': 'b74092de',\n",
    "    'Leicester-City': 'a2d435b3',\n",
    "    'Liverpool': '822bd0ba',\n",
    "    'Manchester-City': 'b8fd03ef',\n",
    "    'Manchester-United': '19538871',\n",
    "    'Newcastle-United': 'b2b47a98',\n",
    "    'Nottingham-Forest': 'e4a775cb',\n",
    "    'Southampton': '33c895d4',\n",
    "    'Tottenham-Hotspur': '361ca564',\n",
    "    'West-Ham': '7c21e445',\n",
    "    'Wolverhampton-Wanderers': '8cec06e1',\n",
    "    }\n",
    "# team_ids.items()\n",
    "season_24 = generate_seasons(2024,2024)   \n",
    "# scrape_all_teams_stats(season_24, team_ids_2024_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Define the login URL and the target URL\n",
    "login_url = \"https://www.superbru.com/login\"\n",
    "target_url = \"https://www.superbru.com/premierleague_predictor/global.php#tab=global-leaderboard\"\n",
    "\n",
    "# Step 2: Start a session\n",
    "session = requests.Session()\n",
    "\n",
    "# Step 3: Get the login page to fetch any CSRF tokens (if applicable)\n",
    "login_page = session.get(login_url)\n",
    "soup = BeautifulSoup(login_page.text, 'html.parser')\n",
    "\n",
    "# Step 4: Prepare \n",
    "#login credentials and form data\n",
    "payload = {\n",
    "    \"username\": \"\",\n",
    "    \"password\": \"\",\n",
    "}\n",
    "\n",
    "# Step 5: Send POST request to login\n",
    "response = session.post(login_url, data=payload)\n",
    "\n",
    "# Check if login was successful\n",
    "if response.status_code == 200 and \"Welcome\" in response.text:  # Adjust based on the website\n",
    "    print(\"Login successful!\")\n",
    "else:\n",
    "    print(\"Login failed!\")\n",
    "    exit()\n",
    "\n",
    "# Step 6: Access the target page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_top_points():\n",
    "    target_url = \"https://www.superbru.com/premierleague_predictor/global.php#tab=global-leaderboard\"\n",
    "    username = \"\",\n",
    "    password = \"\"\n",
    "\n",
    "    # Step 1: Set up WebDriver\n",
    "    driver = webdriver.Chrome()  # Or use another WebDriver\n",
    "    driver.get(target_url)\n",
    "\n",
    "    try:\n",
    "        # Replace 'button.accept-cookies' with the actual selector for the accept button\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"#qc-cmp2-ui > div.qc-cmp2-footer.qc-cmp2-footer-overlay.qc-cmp2-footer-scrolled > div > button.css-13brqst\"))\n",
    "        )\n",
    "        cookie_button.click()\n",
    "        print(\"Cookie popup dismissed.\")\n",
    "    except Exception as e:\n",
    "        print(\"No cookie popup found or error occurred:\", e)\n",
    "\n",
    "    try:\n",
    "        # Adjust the selector to match the desired anchor tag\n",
    "        anchor_tag = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"body > main > div > div > div > div > div > div > div > div.entry > div.tab-bar > ul > li.tab-control.active > a\"))\n",
    "        )\n",
    "        anchor_tag.click()\n",
    "        print(\"Clicked the anchor tag.\")\n",
    "    except Exception as e:\n",
    "        print(\"Anchor tag not found or timeout:\", e)\n",
    "\n",
    "\n",
    "    # Step 2: Log in (if needed)\n",
    "    driver.find_element(By.ID, \"email-superbru\").send_keys(username)\n",
    "    driver.find_element(By.ID, \"password-superbru\").send_keys(password)\n",
    "    driver.find_element(By.XPATH, \"/html/body/main/div/div/div/div/div/div/div/div[3]/div[2]/div[1]/form/div[3]/input\").submit()\n",
    "\n",
    "    # Step 3: Wait for the table to load and scrape it\n",
    "    time.sleep(1)\n",
    "    table = driver.find_element(By.XPATH, \"/html/body/main/div/div[2]/div/div[3]/div[2]/div/div[2]/table\")  # CSS selector for multiple classes\n",
    "    html = table.get_attribute(\"outerHTML\")\n",
    "\n",
    "    # Use BeautifulSoup to parse the table\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows = [[cell.text for cell in row.find_all(\"td\")] for row in soup.find_all(\"tr\")]\n",
    "    print(rows)\n",
    "    top_global_points = rows[2][-1]\n",
    "    top_global_250_points = rows[-1][-1]\n",
    "    print(top_global_points, top_global_250_points)\n",
    "    driver.quit()\n",
    "\n",
    "    return top_global_points, top_global_250_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
